{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import size, udf, unix_timestamp, max, min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, LongType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import array_contains\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load metadata avro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reader = sqlContext.read.format('com.databricks.spark.avro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meta = reader.load('data/spark_metadata.avro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'path',\n",
       " 'message_id',\n",
       " 'date',\n",
       " 'from',\n",
       " 'to',\n",
       " 'cc',\n",
       " 'bcc',\n",
       " 'subject',\n",
       " 'references']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading topic distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topdisFile = 'data/enron_small_topic_distributions.tuples'\n",
    "\n",
    "csvLoader = sqlContext.read.format('com.databricks.spark.csv')\n",
    "topdis = csvLoader.options(delimiter=',',header='false', inferschema='true').load(topdisFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C0',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'C3',\n",
       " 'C4',\n",
       " 'C5',\n",
       " 'C6',\n",
       " 'C7',\n",
       " 'C8',\n",
       " 'C9',\n",
       " 'C10',\n",
       " 'C11',\n",
       " 'C12',\n",
       " 'C13',\n",
       " 'C14',\n",
       " 'C15']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topdis.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "strip_first_col = udf(lambda row: int(row[1:]), IntegerType())\n",
    "topdis = topdis.withColumn('C0',strip_first_col(topdis['C0']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dictionary CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dicFile = 'enron_small_dic.csv'\n",
    "\n",
    "csvLoader = sqlContext.read.format('com.databricks.spark.csv')\n",
    "dic = csvLoader.options(delimiter='\\t', header='false', inferschema='true').load(dicFile)\n",
    "dic = dic.select(dic['C0'].alias('id'), dic['C1'].alias('word'), dic['C2'].alias('count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'word', 'count']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load clustertopics CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clutoFile = 'enron_small_clustertopics.csv'\n",
    "\n",
    "csvLoader = sqlContext.read.format('com.databricks.spark.csv')\n",
    "cluto = csvLoader.options(delimiter=',', header='false', inferschema='true').load(clutoFile)\n",
    "#dic = dic.select(dic['C0'].alias('id'), dic['C1'].alias('word'), dic['C2'].alias('count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C0', 'C1', 'C2', 'C3', 'C4']"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluto.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load topicswords CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "towoFile = 'enron_small_lda_transposed.csv'\n",
    "\n",
    "csvLoader = sqlContext.read.format('com.databricks.spark.csv')\n",
    "towo = csvLoader.options(delimiter=',', header='false', inferschema='true').load(towoFile)\n",
    "#dic = dic.select(dic['C0'].alias('id'), dic['C1'].alias('word'), dic['C2'].alias('count'))'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C0',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'C3',\n",
       " 'C4',\n",
       " 'C5',\n",
       " 'C6',\n",
       " 'C7',\n",
       " 'C8',\n",
       " 'C9',\n",
       " 'C10',\n",
       " 'C11',\n",
       " 'C12',\n",
       " 'C13',\n",
       " 'C14']"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "towo.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge topdis which has document id and with metadata, based on document id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'path',\n",
       " 'message_id',\n",
       " 'date',\n",
       " 'from',\n",
       " 'to',\n",
       " 'cc',\n",
       " 'bcc',\n",
       " 'subject',\n",
       " 'references']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metasmall = meta.select('id',unix_timestamp(meta['date'],\"yyyy-MM-dd'T'HH:mm:ssX\").alias(\"timestamp\"))\n",
    "doctopdat = topdis.join(metasmall, metasmall.id == topdis.C0,'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+\n",
      "|    id|timestamp|\n",
      "+------+---------+\n",
      "|263168|986997480|\n",
      "|263169|988382460|\n",
      "|263170|958665420|\n",
      "|263171|957974880|\n",
      "|263172|982673940|\n",
      "|263173|985683420|\n",
      "|263174|972667500|\n",
      "|263175|988361040|\n",
      "|263176|987674100|\n",
      "|263177|970148880|\n",
      "|263178|968759640|\n",
      "|263179|982778280|\n",
      "|263180|961145160|\n",
      "|263181|990440220|\n",
      "|263182|976064400|\n",
      "|263183|969376680|\n",
      "|263184|962993820|\n",
      "|263185|984742260|\n",
      "|263186|982754280|\n",
      "|263187|960397740|\n",
      "+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metasmall.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "maxdate = doctopdat.select(max('timestamp').alias('maxtimestamp')).collect()[0]['maxtimestamp']\n",
    "mindate = doctopdat.select(min('timestamp').alias('mintimestamp')).collect()[0]['mintimestamp']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1009958530 991169400\n"
     ]
    }
   ],
   "source": [
    "print maxdate, mindate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "binborders =map(int,list(np.linspace(start=mindate,stop=maxdate,num=30,endpoint=False)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1009332225\n"
     ]
    }
   ],
   "source": [
    "print binborders[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#f = udf(lambda x:np.argmax(x < binborders) -1,IntegerType())\n",
    "def f(x):\n",
    "    for i in range(len(binborders)):\n",
    "        if x < binborders[i]:            \n",
    "            return binborders[i-1]\n",
    "    return binborders[-1]\n",
    "    \n",
    "doctopdat = doctopdat.withColumn('bin',udf(f,LongType())(doctopdat['timestamp']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "newdataframe = doctopdat.groupBy('bin').avg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(bin=996179834, avg(C0)=1893.71875, avg(C2)=0.07288942446407053, avg(C3)=0.07596484700417556, avg(C4)=0.06602575028057349, avg(C5)=0.05413960548976752, avg(C6)=0.05225609203825071, avg(C7)=0.07715599872767398, avg(C8)=0.05341275211642137, avg(C9)=0.08024595719927158, avg(C10)=0.06522699142101661, avg(C11)=0.06228288576044858, avg(C12)=0.09235627687222708, avg(C13)=0.05351567203849644, avg(C14)=0.07305290255836651, avg(id)=1893.71875, avg(timestamp)=996539539.90625, avg(bin)=996179834.0)]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdataframe.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "996179834 / (3600*24*7*52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bin',\n",
       " 'avg(C0)',\n",
       " 'avg(C2)',\n",
       " 'avg(C3)',\n",
       " 'avg(C4)',\n",
       " 'avg(C5)',\n",
       " 'avg(C6)',\n",
       " 'avg(C7)',\n",
       " 'avg(C8)',\n",
       " 'avg(C9)',\n",
       " 'avg(C10)',\n",
       " 'avg(C11)',\n",
       " 'avg(C12)',\n",
       " 'avg(C13)',\n",
       " 'avg(C14)',\n",
       " 'avg(id)',\n",
       " 'avg(timestamp)',\n",
       " 'avg(bin)']"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdataframe.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.14959284627 months\n"
     ]
    }
   ],
   "source": [
    "print ((maxdate / (365*24.0*3600)) - (mindate / (365*24.0*3600))) * 12, \"months\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
